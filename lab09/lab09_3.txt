Sameer Mall
Exercise 9.3
04/05


a.    Exercise 1:
       i. Whatâ€™s the size of the cats/dogs datasets?
       The "Dogs vs. Cats" dataset contains 25,000 images however for this exercise a subset of 2000 images is used.
       Both of he training sets have 1000 images and both of the validation sets have 500 images

       ii. How does the first convnet compare with the one we did in class.
       The first Convnet is similar to the one we saw in class. It has 3 modules (conv + relu + maxpooling). The convolutions
       operate on a 3x3 window whereas the maxpooling operates on a 2x2 window. The only thing that's different is that in
       class we used 1 layer of 32 and 2 layers of 64. In this example they use 1 layer of 16 filters, 1 of 32, and one of 64.
       They both also add layers to flatten the 2D image and then do a 10-way vs. 1 way classification.

       iii. Can you see any interesting patterns in the intermediate representations?
       We go from the raw pixels of the images to increasingly abstract and compact representations. The image representations
       carry increasingly less information about the original pixels of the image, but increasingly refined information about the
       class of the image.

       "You can think of a convnet (or a deep network in general) as an information distillation pipeline."

b.    Exercise 2: Reduce over fitting
       i. What is data augmentation?
       A key concern when training a convolutional neural network is overfitting: a model so tuned to the specifics of the
       training data that is it unable to generalize to new examples.
       Data Augmentation: IS A REALLY COOL FEATURE! It artificially boosts the diversity and number of training examples
       by performing random transformations to existing images to create a set of new variants! This is especially useful
       when our dataset is relatively small.

       ii. Report your best results and the hyperparameters you used to get them.
      history = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=30,
      validation_data=validation_generator,
      validation_steps=50,
      verbose=2)

      Using the hyper parameters above there is no more over fitting and the accuracy has increased by around 5 points.
      Accuracy still seems to
      be increasing so we could even use more than 30 epochs. But I did not do that because it took a long time to run.

c.    You can skip Exercise 3.