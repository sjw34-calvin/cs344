Sameer Mall
Exercise 9.1
04/05


a.    How does the linear regression approach to the problem fare?
We first need to train a naive model that uses linear regression. This model uses labels with values in the set {0, 1}
and will try to predict a continuous value that is as close as possible to 0 or 1. We want to interpret the output as a
probability, so it would be ideal if the output was between 0 and 1. We would then apply a threshold of 0.5 to determine
the label.
The final RMSE on training data is 0.45 which is below the threshold of 0.5. The value is also between 0 and 1 which means
it can be interpreted as a probability. The validation data follows a similar pattern to the training data.

b.    Task 1: Compare and contrast L2 Loss vs LogLoss.
Linear regression uses the L2 loss. This doesn't do a great job at penalizing the misclassifications when the output is a
probability. L2 does not differentiate whether a negative example is classified as a positive.
In contrast, LogLoss penalizes these "confidence errors" much more heavily.

c.    Task 2: Explain how the logistic regression fares compared to the linear regression.
At the moment LogLoss appears to be worse than the linear regression because I obtained a LogLoss value of 0.54, which is
above the 0.5 threshold value.

d.    Task 3: Here, just report the best values you can achieve for AUC/accuracy and what hyperparameters you used to get them.


linear_classifier = train_linear_classifier_model(
    learning_rate=0.000003,
    steps=20000,
    batch_size=500,
    training_examples=training_examples,
    training_targets=training_targets,
    validation_examples=validation_examples,
    validation_targets=validation_targets)

I noticed that having a higher number of steps up until a certain point leads to a greater AUC and accuracy which are below:
AUC on the validation set: 0.80
Accuracy on the validation set: 0.78

